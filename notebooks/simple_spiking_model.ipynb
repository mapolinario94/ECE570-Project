{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://{TOKEN}@github.com/mapolinario94/ECE570-Project.git"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/content/ECE570-Project/models')\n",
    "sys.path.insert(0,'/content/ECE570-Project/models/spiking_layers.py')\n",
    "sys.path.insert(0,'/content/ECE570-Project/models/conversion_methods.py')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import timeit\n",
    "import math\n",
    "try:\n",
    "    from spiking_layers import LinearLIF, Conv2dLIF\n",
    "    from conversion_method import SpikeNorm\n",
    "except:\n",
    "    from models.spiking_layers import LinearLIF, Conv2dLIF\n",
    "    from models.conversion_method import SpikeNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                      torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10('/data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10('/data', train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size_train, batch_size_test = 64, 1000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(classifier, epoch, optimizer):\n",
    "\n",
    "  classifier.train() # we need to set the mode for our model\n",
    "\n",
    "  for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "    images, targets = images.to(DEVICE), targets.to(DEVICE)\n",
    "    optimizer.zero_grad()\n",
    "    output = classifier(images)\n",
    "    loss = F.cross_entropy(output, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_idx % 10 == 0: # We record our output every 10 batches\n",
    "      train_losses.append(loss.item()) # item() is to get the value of the tensor directly\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "    if batch_idx % 300 == 0: # We visulize our output every 10 batches\n",
    "      print(f'Epoch {epoch}: [{batch_idx*len(images)}/{len(train_loader.dataset)}] Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "def test(classifier, epoch):\n",
    "\n",
    "  classifier.eval() # we need to set the mode for our model\n",
    "\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "      images, targets = images.to(DEVICE), targets.to(DEVICE)\n",
    "      output = classifier(images)\n",
    "      test_loss += F.cross_entropy(output, targets, reduction='sum').item()\n",
    "      pred = output.data.max(1, keepdim=True)[1] # we get the estimate of our result by look at the largest class value\n",
    "      correct += pred.eq(targets.data.view_as(pred)).sum() # sum up the corrected samples\n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  test_counter.append(len(train_loader.dataset)*epoch)\n",
    "\n",
    "  print(f'Test result on epoch {epoch}: Avg loss is {test_loss}, Accuracy: {100.*correct/len(test_loader.dataset)}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SpikingModel(nn.Module):\n",
    "  def __init__(self, device=None):\n",
    "    super(SpikingModel, self).__init__()\n",
    "    self.features = nn.Sequential(\n",
    "        Conv2dLIF(3, 32, kernel_size=5, padding='same', device=device, leak=1.0),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        Conv2dLIF(32, 64, kernel_size=5, padding='same', device=device, leak=1.0),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        Conv2dLIF(64, 64, kernel_size=3, padding='same', device=device, leak=1.0)\n",
    "        )\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "        LinearLIF(8*8*64, 256, device=device, leak=1.0),\n",
    "        nn.ReLU(inplace=True),\n",
    "        LinearLIF(256, 10, cumulative=True, device=device, leak=1.0)\n",
    "        )\n",
    "\n",
    "    self._init_weights()\n",
    "\n",
    "  def _init_internal_states(self):\n",
    "    self.mem_classifier = []\n",
    "    self.mem_features = []\n",
    "    for layer_idx in range(len(self.classifier)):\n",
    "      self.mem_classifier += [None]\n",
    "    for layer_idx in range(len(self.features)):\n",
    "      self.mem_features += [None]\n",
    "\n",
    "  def _init_weights(self):\n",
    "    for m in self.modules():\n",
    "      if isinstance(m, nn.Conv2d):\n",
    "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        m.weight.data.normal_(0,  math.sqrt(2. / n))\n",
    "        if m.bias is not None:\n",
    "          m.bias.data.zero_()\n",
    "      elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "      elif isinstance(m, nn.Linear):\n",
    "        n = m.weight.size(1)\n",
    "        m.weight.data.normal_(0, 0.01)\n",
    "        if m.bias is not None:\n",
    "          m.bias.data.zero_()\n",
    "\n",
    "  def forward(self, X):\n",
    "    batch_size = X.shape[0]\n",
    "    self._init_internal_states()\n",
    "    for t in range(10):\n",
    "      spk = X\n",
    "      for layer_idx in range(len(self.features)):\n",
    "        if isinstance(self.features[layer_idx], nn.Conv2d):\n",
    "          spk, self.mem_features[layer_idx] = self.features[layer_idx](spk, self.mem_features[layer_idx])\n",
    "          # print(layer_idx, \"conv\")\n",
    "        if isinstance(self.features[layer_idx], nn.MaxPool2d):\n",
    "          spk = self.features[layer_idx](spk)\n",
    "          # print(layer_idx, \"pool\")\n",
    "\n",
    "      spk = spk.view(batch_size, -1)\n",
    "      for layer_idx in range(len(self.classifier)):\n",
    "        if isinstance(self.classifier[layer_idx], nn.Linear):\n",
    "          spk, self.mem_classifier[layer_idx] = self.classifier[layer_idx](spk, self.mem_classifier[layer_idx])\n",
    "          # print(layer_idx, \"linear\")\n",
    "    return self.mem_classifier[layer_idx]\n",
    "\n",
    "snn_model = SpikingModel(DEVICE)\n",
    "snn_model.to(DEVICE)\n",
    "\n",
    "optimizer_snn = optim.Adam(snn_model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = []\n",
    "max_epoch = 6\n",
    "\n",
    "def training_snn():\n",
    "  for epoch in range(1, max_epoch+1):\n",
    "    train(snn_model, epoch, optimizer_snn)\n",
    "    test(snn_model, epoch)\n",
    "\n",
    "# print(f'Total time for CNN: {timeit.timeit(training_snn, number=1)/60} min')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ANNModel, self).__init__()\n",
    "    self.features = nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=5, padding='same', bias=False),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(32, 64, kernel_size=5, padding='same', bias=False),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, padding='same', bias=False)\n",
    "    )\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(8*8*64, 256, bias=False),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(256, 10, bias=False)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, X):\n",
    "    batch_size = X.shape[0]\n",
    "    x = self.features(X)\n",
    "    x = x.view(batch_size, -1)\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "\n",
    "ann_model = ANNModel()\n",
    "ann_model.to(DEVICE)\n",
    "\n",
    "optimizer_ann = optim.Adam(ann_model.parameters(), lr=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = []\n",
    "max_epoch = 20\n",
    "\n",
    "def training_ann():\n",
    "  for epoch in range(1, max_epoch+1):\n",
    "    train(ann_model, epoch, optimizer_ann)\n",
    "    test(ann_model, epoch)\n",
    "\n",
    "print(f'Total time for CNN: {timeit.timeit(training_ann, number=1)/60} min')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_cifar_cnn = SpikeNorm(ann_model, snn_model, train_loader, DEVICE, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}